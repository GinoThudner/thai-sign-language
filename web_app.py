import streamlit as st

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠ SEO ---
st.set_page_config(
    page_title="‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå - AI Sign Language Translator",
    page_icon="üñêÔ∏è",
    layout="centered"
)

import cv2
import mediapipe as mp
import pickle
import numpy as np
import os
import pandas as pd
import copy
import itertools
import queue
import time
from streamlit_webrtc import webrtc_streamer, WebRtcMode

# --- 2. ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ ---
st.title("üñêÔ∏è ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö Real-time")
st.markdown("---")

# --- 3. ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(BASE_DIR, 'keypoint_classifier_model.pkl')
label_path = os.path.join(BASE_DIR, 'keypoint_classifier_label.csv')

result_queue = queue.Queue()

@st.cache_resource
def load_resources():
    with open(model_path, 'rb') as f:
        m = pickle.load(f)
        model_obj = m['model'] if isinstance(m, dict) else m
    
    if os.path.exists(label_path):
        df = pd.read_csv(label_path, header=None, encoding='utf-8')
        labels_list = df.iloc[:, 1].astype(str).tolist() if df.shape[1] > 1 else df.iloc[:, 0].astype(str).tolist()
    else:
        labels_list = ["Error: No Label File"]
    
    mp_hands = mp.solutions.hands
    # ‡∏õ‡∏£‡∏±‡∏ö min_detection_confidence ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    hands_engine = mp_hands.Hands(
        max_num_hands=2, 
        min_detection_confidence=0.5, 
        min_tracking_confidence=0.5
    )
    return model_obj, labels_list, hands_engine, mp.solutions.drawing_utils, mp_hands

model, labels, hands, mp_draw, mp_hands_module = load_resources()

# --- 4. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏ö‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ---
def pre_process_landmark(landmark_list):
    temp_landmark_list = copy.deepcopy(landmark_list)
    base_x, base_y = temp_landmark_list[0][0], temp_landmark_list[0][1]
    for i in range(len(temp_landmark_list)):
        temp_landmark_list[i][0] -= base_x
        temp_landmark_list[i][1] -= base_y
    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))
    max_val = max(list(map(abs, temp_landmark_list)))
    return [n / max_val if max_val != 0 else 0 for n in temp_landmark_list]

def flip_keypoint_x(keypoint_list):
    flipped = list(keypoint_list)
    for i in range(0, 42, 2): flipped[i] *= -1
    return flipped

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÑ‡∏ß‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Frame Skipping (‡∏•‡∏î‡∏†‡∏≤‡∏£‡∏∞ CPU)
last_process_time = 0

def video_frame_callback(frame):
    global last_process_time
    # 1. ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    img = frame.to_ndarray(format="bgr24")
    img = cv2.flip(img, 1) # ‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡∏à‡∏Å
    
    current_time = time.time()
    
    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ CPU ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    if current_time - last_process_time > 0.1:
        last_process_time = current_time
        h, w, _ = img.shape
        
        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ img_rgb ‡∏Å‡πà‡∏≠‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        # 4. ‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏´‡πâ AI ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        results = hands.process(img_rgb)

        if results.multi_hand_landmarks:
            for hl in results.multi_hand_landmarks:
                mp_draw.draw_landmarks(img, hl, mp_hands_module.HAND_CONNECTIONS)
            
            # --- ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ---
            data_aux = []
            # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡∏∂‡∏á Landmark ‡πÅ‡∏•‡∏∞ Predict ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì) ...
            # ---------------------------

    # 5. ‡∏™‡πà‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ß‡∏≤‡∏î‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÇ‡∏ä‡∏ß‡πå‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
    return frame.from_ndarray(img, format="bgr24")

# --- 5. ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÄ‡∏ß‡πá‡∏ö ---
output_container = st.empty()
output_container.success("üí° ‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö...")

webrtc_streamer(
    key="thai-sign-v3", # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Key ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏Ñ‡πâ‡∏≤‡∏á
    mode=WebRtcMode.SENDRECV,
    rtc_configuration={
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}],
        "iceTransportPolicy": "all",
    },
    video_frame_callback=video_frame_callback,
    media_stream_constraints={
        "video": {
            "width": {"ideal": 320}, # ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏•‡∏á‡∏≠‡∏µ‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏•‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
            "height": {"ideal": 240},
            "frameRate": {"ideal": 15}
        },
        "audio": False
    },
    async_processing=True,
)

# ‡∏•‡∏π‡∏õ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏°‡∏≤‡πÇ‡∏ä‡∏ß‡πå
while True:
    try:
        msg = result_queue.get(timeout=1.0)
        output_container.markdown(
            f"""
            <div style="background-color: #d4edda; color: #155724; padding: 15px; border-radius: 10px; text-align: center;">
                <p style="margin: 0; font-size: 18px;">‚úÖ ‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö:</p>
                <h1 style="margin: 0; font-size: 60px; font-weight: bold;">{msg}</h1>
            </div>
            """,
            unsafe_allow_html=True
        )
    except queue.Empty:
        pass



