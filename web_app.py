import streamlit as st

# 1. ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î
st.set_page_config(page_title="Thai Sign Translator", layout="centered")

import cv2
import mediapipe as mp
import pickle
import numpy as np
import os
import pandas as pd
import copy
import itertools
from streamlit_webrtc import webrtc_streamer, WebRtcMode

# --- 2. ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ (‡πÉ‡∏ä‡πâ Cache ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡∏ã‡πâ‡∏≥‡∏à‡∏ô‡∏Ñ‡πâ‡∏≤‡∏á) ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(BASE_DIR, 'keypoint_classifier_model.pkl')
label_path = os.path.join(BASE_DIR, 'keypoint_classifier_label.csv')

@st.cache_resource
def load_resources():
    with open(model_path, 'rb') as f:
        m = pickle.load(f)
        model_obj = m['model'] if isinstance(m, dict) else m
    
    if os.path.exists(label_path):
        df = pd.read_csv(label_path, header=None, encoding='utf-8')
        labels_list = df.iloc[:, 1].astype(str).tolist() if df.shape[1] > 1 else df.iloc[:, 0].astype(str).tolist()
    else:
        labels_list = ["No Label"]
    
    mp_hands = mp.solutions.hands
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏´‡πâ‡∏û‡∏≠‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    hands_engine = mp_hands.Hands(
        max_num_hands=1, # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÅ‡∏Ñ‡πà 1 ‡∏°‡∏∑‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î CPU
        min_detection_confidence=0.5, 
        min_tracking_confidence=0.5
    )
    return model_obj, labels_list, hands_engine, mp.solutions.drawing_utils, mp_hands

model, labels, hands, mp_draw, mp_hands_module = load_resources()

# --- 3. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß) ---
def pre_process_landmark(landmark_list):
    temp_landmark_list = copy.deepcopy(landmark_list)
    base_x, base_y = temp_landmark_list[0][0], temp_landmark_list[0][1]
    for i in range(len(temp_landmark_list)):
        temp_landmark_list[i][0] -= base_x
        temp_landmark_list[i][1] -= base_y
    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))
    max_val = max(list(map(abs, temp_landmark_list)))
    return [n / max_val if max_val != 0 else 0 for n in temp_landmark_list]

def video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    img = cv2.flip(img, 1) # ‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏û
    h, w, _ = img.shape
    
    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏°‡∏∑‡∏≠
    results = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

    if results.multi_hand_landmarks:
        for hl in results.multi_hand_landmarks:
            # ‡∏ß‡∏≤‡∏î‡πÄ‡∏™‡πâ‡∏ô‡∏°‡∏∑‡∏≠‡∏î‡πâ‡∏ß‡∏¢ OpenCV (‡πÄ‡∏ö‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
            mp_draw.draw_landmarks(img, hl, mp_hands_module.HAND_CONNECTIONS)
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            pts = [[int(l.x * w), int(l.y * h)] for l in hl.landmark]
            processed = pre_process_landmark(pts)
            
            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (‡πÉ‡∏™‡πà 0.0 ‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 84 features ‡∏ï‡∏≤‡∏°‡∏™‡∏π‡∏ï‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)
            input_data = processed + ([0.0] * 42)
            prediction = model.predict(np.array([input_data[:84]]))[0]
            conf = model.predict_proba(np.array([input_data[:84]])).max()
            
            if conf > 0.8:
                res_thai = labels[int(prediction)]
                # ‡πÅ‡∏™‡∏î‡∏á ID ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÉ‡∏ô‡∏à‡∏≠ (‡∏Å‡∏±‡∏ô‡∏Ñ‡πâ‡∏≤‡∏á)
                cv2.rectangle(img, (0, h-40), (200, h), (0,0,0), -1)
                cv2.putText(img, f"ID: {prediction} ({conf:.2f})", (10, h-10), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
                # ‡∏ù‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏Ñ‡∏¥‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏µ‡πà UI ‡∏´‡∏•‡∏±‡∏Å
    
    return frame.from_ndarray(img, format="bgr24")

# --- 4. ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö ---
st.title("üñêÔ∏è ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢")
st.info("üí° ‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ñ‡∏ö‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ID ‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠")

# ‡πÉ‡∏ä‡πâ WebRTC Streamer ‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£
ctx = webrtc_streamer(
    key="fixed-final",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    video_frame_callback=video_frame_callback,
    media_stream_constraints={"video": True, "audio": False},
    async_processing=True, # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏≤‡∏á
)

# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ö‡∏ô‡∏à‡∏≠‡∏Å‡∏•‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ö‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Cloud ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏≤‡∏á 
# ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ñ‡∏ö Success ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÄ‡∏•‡∏Ç ID ‡∏ö‡∏ô‡∏à‡∏≠‡πÅ‡∏ó‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö
