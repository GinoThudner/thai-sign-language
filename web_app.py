import streamlit as st
import cv2
import mediapipe as mp
import pickle
import numpy as np
import os
import pandas as pd
import copy
import itertools
import queue
import collections # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏û‡∏¥‡∏Å‡∏±‡∏î
from streamlit_webrtc import webrtc_streamer, WebRtcMode

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠ SEO ---
st.set_page_config(
    page_title="‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå - AI Sign Language Translator",
    page_icon="üñêÔ∏è",
    layout="centered"
)

# --- 2. ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (Motion History) ---
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡∏û‡∏¥‡∏Å‡∏±‡∏î 15 ‡πÄ‡∏ü‡∏£‡∏°‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏±‡∏ö
if "history" not in st.session_state:
    st.session_state.history = collections.deque(maxlen=15)

# --- 3. ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ ---
st.title("üñêÔ∏è ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö Real-time")
st.markdown("""
### ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ô‡∏¥‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß
* **‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ô‡∏¥‡πà‡∏á:** ‡πÉ‡∏ä‡πâ AI ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥
* **‡∏Ç‡∏¢‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡∏ã‡πâ‡∏≤‡∏¢-‡∏Ç‡∏ß‡∏≤:** ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ **"‡πÑ‡∏°‡πà"**
* **‡∏°‡∏∑‡∏≠‡∏ô‡∏¥‡πà‡∏á‡∏™‡∏ô‡∏¥‡∏ó:** ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ **"‡∏´‡∏¢‡∏∏‡∏î"**
""")

# --- 4. ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(BASE_DIR, 'keypoint_classifier_model.pkl')
label_path = os.path.join(BASE_DIR, 'keypoint_classifier_label.csv')

result_queue = queue.Queue()

@st.cache_resource
def load_resources():
    with open(model_path, 'rb') as f:
        m = pickle.load(f)
        model_obj = m['model'] if isinstance(m, dict) else m
    
    if os.path.exists(label_path):
        df = pd.read_csv(label_path, header=None, encoding='utf-8')
        labels_list = df.iloc[:, 1].astype(str).tolist() if df.shape[1] > 1 else df.iloc[:, 0].astype(str).tolist()
    else:
        labels_list = ["Error: No Label File"]
    
    mp_hands = mp.solutions.hands
    hands_engine = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)
    return model_obj, labels_list, hands_engine, mp.solutions.drawing_utils, mp_hands

model, labels, hands, mp_draw, mp_hands_module = load_resources()

# --- 5. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ---
def pre_process_landmark(landmark_list):
    temp_landmark_list = copy.deepcopy(landmark_list)
    base_x, base_y = temp_landmark_list[0][0], temp_landmark_list[0][1]
    for i in range(len(temp_landmark_list)):
        temp_landmark_list[i][0] -= base_x
        temp_landmark_list[i][1] -= base_y
    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))
    max_val = max(list(map(abs, temp_landmark_list)))
    return [n / max_val if max_val != 0 else 0 for n in temp_landmark_list]

def flip_keypoint_x(keypoint_list):
    flipped = list(keypoint_list)
    for i in range(0, 42, 2): flipped[i] *= -1
    return flipped

def video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    img = cv2.flip(img, 1)
    h, w, _ = img.shape
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    results = hands.process(img_rgb)

    if results.multi_hand_landmarks:
        # ‡∏î‡∏∂‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 9 (‡πÇ‡∏Ñ‡∏ô‡∏ô‡∏¥‡πâ‡∏ß‡∏Å‡∏•‡∏≤‡∏á) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Motion
        p9 = results.multi_hand_landmarks[0].landmark[9]
        st.session_state.history.append((p9.x, p9.y))

        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Motion (‡πÑ‡∏°‡πà/‡∏´‡∏¢‡∏∏‡∏î) ---
        motion_detected = False
        if len(st.session_state.history) == 15:
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á X ‡πÅ‡∏•‡∏∞ Y
            dx = st.session_state.history[-1][0] - st.session_state.history[0][0]
            dy = st.session_state.history[-1][1] - st.session_state.history[0][1]
            speed = (dx**2 + dy**2)**0.5

            if abs(dx) > 0.12: # ‡∏™‡πà‡∏≤‡∏¢‡∏°‡∏∑‡∏≠‡∏ã‡πâ‡∏≤‡∏¢‡∏Ç‡∏ß‡∏≤‡πÅ‡∏£‡∏á‡∏û‡∏≠
                result_queue.put("‡πÑ‡∏°‡πà")
                motion_detected = True
            elif speed < 0.005: # ‡∏°‡∏∑‡∏≠‡∏ô‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å‡∏à‡∏£‡∏¥‡∏á‡πÜ
                result_queue.put("‡∏´‡∏¢‡∏∏‡∏î")
                motion_detected = True

        # --- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏±‡∏ö‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ AI ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏õ‡∏Å‡∏ï‡∏¥ ---
        if not motion_detected:
            for hl in results.multi_hand_landmarks:
                mp_draw.draw_landmarks(img, hl, mp_hands_module.HAND_CONNECTIONS)
            
            data_aux = []
            sorted_hands = sorted(zip(results.multi_hand_landmarks, results.multi_handedness),
                                  key=lambda x: x[0].landmark[0].x)
            
            if len(sorted_hands) == 1:
                hl, hn = sorted_hands[0]
                pts = [[int(l.x * w), int(l.y * h)] for l in hl.landmark]
                processed = pre_process_landmark(pts)
                if hn.classification[0].label == 'Right':
                    processed = flip_keypoint_x(processed)
                data_aux.extend(processed)
                data_aux.extend([0.0] * 42)
            elif len(sorted_hands) >= 2:
                for i in range(2):
                    hl = sorted_hands[i][0]
                    pts = [[int(l.x * w), int(l.y * h)] for l in hl.landmark]
                    data_aux.extend(pre_process_landmark(pts))
            
            if len(data_aux) == 84:
                prediction = model.predict(np.array([data_aux]))[0]
                conf = model.predict_proba(np.array([data_aux])).max()
                if conf > 0.6:
                    result_queue.put(labels[int(prediction)])

    return frame.from_ndarray(img, format="bgr24")

# --- 6. ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÄ‡∏ß‡πá‡∏ö ---
output_container = st.empty()

webrtc_streamer(
    key="motion-detect-v1",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    video_frame_callback=video_frame_callback,
    media_stream_constraints={
        "video": {
            "width": {"exact": 640}, 
            "height": {"exact": 480}, 
            "frameRate": {"ideal": 15}
        },
        "audio": False
    },
    async_processing=True,
)

while True:
    try:
        msg = result_queue.get(timeout=1.0)
        output_container.markdown(
            f"""
            <div style="background-color: #d4edda; color: #155724; padding: 20px; border-radius: 10px; border: 1px solid #c3e6cb; text-align: center;">
                <p style="margin: 0; font-size: 20px;">‚úÖ ‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö:</p>
                <h1 style="margin: 0; font-size: 70px; font-weight: bold;">{msg}</h1>
            </div>
            """,
            unsafe_allow_html=True
        )
    except queue.Empty:
        pass
