import streamlit as st
import cv2
import mediapipe as mp
import pickle
import numpy as np
import os
import pandas as pd
import collections
import copy
import itertools
from streamlit_webrtc import webrtc_streamer, WebRtcMode

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ---
st.set_page_config(page_title="Sign Language AI", layout="centered")

if "last_msg" not in st.session_state:
    st.session_state.last_msg = "‡∏£‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö..."
if "history" not in st.session_state:
    st.session_state.history = collections.deque(maxlen=10)

# --- 2. ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(BASE_DIR, 'keypoint_classifier_model.pkl')
label_path = os.path.join(BASE_DIR, 'keypoint_classifier_label.csv')

@st.cache_resource
def load_resources():
    with open(model_path, 'rb') as f:
        m = pickle.load(f)
        model_obj = m['model'] if isinstance(m, dict) else m
    
    # ‡∏≠‡πà‡∏≤‡∏ô Label ‡πÇ‡∏î‡∏¢‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    df = pd.read_csv(label_path, header=None, encoding='utf-8')
    labels_list = df.iloc[:, -1].astype(str).tolist()
    
    mp_hands = mp.solutions.hands
    hands_engine = mp_hands.Hands(
        max_num_hands=1, 
        min_detection_confidence=0.3, # ‡∏•‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏á‡∏ô‡πâ‡∏≠‡∏¢
        min_tracking_confidence=0.3
    )
    return model_obj, labels_list, hands_engine, mp.solutions.drawing_utils, mp_hands

model, labels, hands, mp_draw, mp_hands_module = load_resources()

# --- 3. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¥‡∏Å‡∏±‡∏î ---
def pre_process_landmark(landmark_list):
    temp_landmark_list = copy.deepcopy(landmark_list)
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô (0,0)
    base_x, base_y = temp_landmark_list[0][0], temp_landmark_list[0][1]
    for i in range(len(temp_landmark_list)):
        temp_landmark_list[i][0] -= base_x
        temp_landmark_list[i][1] -= base_y
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))
    # Normalization
    max_val = max(list(map(abs, temp_landmark_list)))
    return [n / max_val if max_val != 0 else 0 for n in temp_landmark_list]

# --- 4. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ---
def video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    img = cv2.flip(img, 1)
    h, w, _ = img.shape
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    results = hands.process(img_rgb)

    if results.multi_hand_landmarks:
        hl = results.multi_hand_landmarks[0]
        mp_draw.draw_landmarks(img, hl, mp_hands_module.HAND_CONNECTIONS)
        
        # ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Motion (‡∏™‡πà‡∏≤‡∏¢‡∏°‡∏∑‡∏≠)
        p9 = hl.landmark[9]
        st.session_state.history.append((p9.x, p9.y))

        if len(st.session_state.history) == 10:
            dx = st.session_state.history[-1][0] - st.session_state.history[0][0]
            # ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏≤‡∏¢‡∏°‡∏∑‡∏≠‡∏ã‡πâ‡∏≤‡∏¢-‡∏Ç‡∏ß‡∏≤
            if abs(dx) > 0.12: 
                st.session_state.last_msg = "‡πÑ‡∏°‡πà"
                return frame.from_ndarray(img, format="bgr24")

        # ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÉ‡∏ä‡πâ AI ‡πÅ‡∏õ‡∏•‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ô‡∏¥‡πà‡∏á
        landmark_list = [[int(l.x * w), int(l.y * h)] for l in hl.landmark]
        processed_data = pre_process_landmark(landmark_list)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•
        input_data = np.array([processed_data], dtype=np.float32)
        prediction = model.predict(input_data)[0]
        conf = model.predict_proba(input_data).max()

        if conf > 0.6:
            st.session_state.last_msg = labels[int(prediction)]

    return frame.from_ndarray(img, format="bgr24")

# --- 5. ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• UI ---
st.title("üñêÔ∏è ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢")

# ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•
placeholder = st.empty()
placeholder.markdown(
    f"""
    <div style="background-color: #2b2b2b; color: #00ff00; padding: 20px; border-radius: 15px; border: 3px solid #00ff00; text-align: center; margin-bottom: 10px;">
        <h2 style="margin: 0; font-size: 1.5rem;">‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö:</h2>
        <h1 style="margin: 0; font-size: 5rem; font-weight: bold;">{st.session_state.last_msg}</h1>
    </div>
    """,
    unsafe_allow_html=True
)

# ‡∏™‡∏ï‡∏£‡∏µ‡∏°‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Glitch
webrtc_streamer(
    key="fixed-final-v1",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    video_frame_callback=video_frame_callback,
    media_stream_constraints={
        "video": {
            "width": {"ideal": 320}, 
            "height": {"ideal": 240}, 
            "frameRate": {"ideal": 12}
        },
        "audio": False
    },
    async_processing=True,
)

if st.button("‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä / ‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•"):
    st.session_state.last_msg = "‡∏£‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö..."
    st.rerun()
